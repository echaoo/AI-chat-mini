 陪伴类产品中至关重要也极具挑战的模块。完全的“记忆”是不现实的（因为会无限拉长上下文，导致成本和延迟飙升），所以我们需要设计巧妙的、分层的记忆机制。

  这里有三种实现策略，从简单到复杂，您可以根据自己的技术栈和期望效果来选择：

  策略一：滚动摘要式记忆（简单且高效）

  这是最推荐的入门方案，性价比最高。

   * 核心思想: 不保留所有对话历史，而是在每次对话结束后，用 LLM 自身的能力生成一个“摘要”，这个摘要会成为下一次对话的“前情提要”。

   * 实现步骤:
       1. 数据存储: 在您的数据库中，为每个对话配对（例如 user_id 和 character_id）增加一个字段，比如 memory_summary (TEXT类型)。
       2. 生成摘要: 当一次对话结束时（比如用户离开聊天页面，或者对话超过N轮），触发一个后端任务：
           * 取出最近的 M 条对话记录（例如最近的 20 条）。
           * 调用一次 LLM API，使用一个专门的 Prompt 来生成摘要。例如：

   1             请将以下用户与AI角色的对话内容，浓缩成一段简短的摘要。摘要应包含对话的关键信息、用户的情绪状态和任何重要约定。请使用第三人称视角。
   2 
   3             对话历史:
   4             {{last_20_messages}}
   5 
   6             摘要:
           * 将 LLM 返回的摘要更新到数据库的 memory_summary 字段中。如果已有旧摘要，可以把新摘要和旧摘要结合再做一次摘要，实现“滚动更新”。
       3. 使用记忆: 当用户开始一次新的对话时：
           * 后端从数据库中取出 memory_summary。
           * 将摘要内容作为背景信息，插入到主对话的 System Prompt 中。例如：

   1             你是一个AI角色。以下是你和用户过去的对话摘要，请在此基础上继续你们的对话，并表现出你记得这些事：
   2             "{{memory_summary}}"
   3 
   4             现在，开始新的对话。

   * 优点: 实现相对简单，有效控制了上下文长度和 API 成本，能很好地制造“短期记忆”和“中期记忆”的印象。
   * 缺点: 细节会丢失，太久远的记忆会被“滚雪球”式地“忘掉”。

  策略二：关键信息提取 + 向量搜索（长期记忆的进阶方案）

  当您希望 AI 能记住非常久远且具体的细节时，这个方案更优越。

   * 核心思想: 将对话中的“关键信息”提取为独立的“记忆碎片”，存入向量数据库。当新对话发生时，通过语义搜索找到最“相关”的记忆碎片并注入。

   * 实现步骤:
       1. 提取记忆碎片: 在对话过程中或结束后，通过 LLM 提取结构化的“事实”。
           * Prompt 示例:

   1             从以下对话中，抽取出关于用户本人的关键信息（例如：姓名、职业、爱好、最近的困扰、重要的人或事）。请以JSON数组的形式返回，每条信息是一个字符串。
   2 
   3             对话历史:
   4             {{conversation_snippet}}
   5 
   6             输出:
   7             ["用户的猫叫'咪咪'", "用户最近在准备一场重要的考试", "用户喜欢在下雨天听音乐"]
       2. 存储记忆碎片:
           * 对每一个提取出的信息字符串（“用户的猫叫'咪咪'”），使用 Embedding 模型（例如 OpenAI 的 atext-embedding-3-small）将其转换为一个向量（一长串数字）。
           * 将原始信息字符串和其向量表示一起存入向量数据库（如 Pinecone, ChromaDB, pgvector 等），并与 user_id 和 character_id 关联。
       3. 检索与使用:
           * 当用户发来新消息时（例如：“我的猫好像生病了”），后端同样将这条新消息通过 Embedding 模型转换为向量。
           * 用这个“查询向量”去向量数据库中进行相似度搜索，找出最匹配的 K 个历史记忆碎片（它很可能会找到“用户的猫叫'咪咪'”）。
           * 将这些检索到的记忆碎片注入到 Prompt 中：

   1             背景知识：以下是你和用户的一些相关记忆：
   2             - {{retrieved_memory_1}}
   3             - {{retrieved_memory_2}}
   4             请运用这些记忆，继续你们的对话。

   * 优点: 能实现精准、长期的“联想式”记忆，扩展性极强。
   * 缺点: 技术栈更复杂，需要引入向量数据库和 Embedding 服务，成本更高。

  策略三：混合模式（最终形态）

  结合策略一和策略二的优点。

   * 实现: 同时使用两种策略。
       * 滚动摘要作为“通用背景记忆”，让 AI 对最近的整体对话氛围有印象。
       * 向量搜索作为“精准细节记忆”，在特定话题上实现“灵光一闪”般的精准回忆。
   * Prompt 示例:

   1     你是一个AI角色。
   2     # 通用记忆 (最近的对话摘要):
   3     "{{memory_summary}}"
   4 
   5     # 相关细节记忆 (根据当前输入检索到的):
   6     - {{retrieved_memory_1}}
   7     - {{retrieved_memory_2}}
   8 
   9     请综合以上所有信息，自然地继续你们的对话。

  我的建议

   1. 从“策略一：滚动摘要式记忆”开始做起。 它的实现成本最低，但效果提升最明显，能快速验证您的想法。
   2. 当您发现用户对“长期细节记忆”有更强烈的需求时，再考虑升级到“策略二”或“策略三”

    
    技术补充

    这是一个设计良好的记忆方案，三层递进的架构很合理。从技术角度，我来补充一些需要考虑的点：

  ---
  技术评估与补充建议

  一、策略一的细节补充

  1. 触发时机需要更精确

  文档提到"用户离开聊天页面"，但在小程序中这不够可靠：

  场景问题：
  - onHide（后台）vs onUnload（关闭）行为不同
  - 网络中断、App 被杀死时无法触发
  - 用户快速切换页面可能重复触发

  建议方案：
  - 双重触发：消息轮次（如每10轮）+ 时间间隔（如30分钟无活动）
  - 后端定时任务兜底扫描"未摘要"的对话
  - 使用消息队列异步处理，避免阻塞

  2. 滚动摘要的信息衰减问题

  "新摘要+旧摘要再做一次摘要"会导致信息持续丢失，建议：

  分层摘要架构：
  ├── 短期记忆：最近 20 条原始消息
  ├── 中期记忆：近 7 天的日摘要
  └── 长期记忆：更早的周/月摘要

  合并策略：
  - 设置摘要最大 token 数（如 500 tokens）
  - 超出时才触发"摘要的摘要"
  - 保留关键实体（人名、日期、承诺）的优先级更高

  3. 并发与幂等性

  // 需要防止的问题
  // 用户快速发多条消息 → 多次触发摘要 → 覆盖或重复

  // 建议方案
  interface SummaryTask {
    conversationId: number
    version: number      // 乐观锁
    status: 'pending' | 'processing' | 'done'
    lastMessageId: number // 记录到哪条消息
  }

  ---
  二、策略二的技术细节

  1. 记忆碎片的生命周期管理

  文档未提及记忆碎片的更新和失效：

  问题场景：
  - 用户说"我养了一只猫叫咪咪" → 存储
  - 半年后用户说"咪咪去世了" → 旧记忆需要更新或标记失效

  建议：
  - 记忆碎片带时间戳和置信度
  - 检测冲突信息时触发更新
  - 定期清理低置信度/长期未命中的碎片

  2. 向量数据库选型建议

  | 方案     | 优点                                 | 缺点               |
  |----------|--------------------------------------|--------------------|
  | pgvector | 与现有 PostgreSQL 无缝集成，运维简单 | 大规模性能一般     |
  | Pinecone | 托管服务，开箱即用                   | 成本高，数据出境   |
  | ChromaDB | 轻量级，适合原型                     | 生产环境稳定性一般 |
  | Milvus   | 高性能，开源                         | 运维复杂           |

  对于你当前项目，建议 pgvector，因为你已经用了 PostgreSQL。

  3. Embedding 模型选择

  推荐：
  - text-embedding-3-small：性价比高，1536维
  - 中文场景可考虑：BGE-M3、text2vec-large-chinese

  成本估算（OpenAI）：
  - ~$0.02 / 1M tokens
  - 每条记忆碎片约 20-50 tokens
  - 1000 条记忆 ≈ $0.001

  ---
  三、文档未覆盖的关键问题

  1. 失败处理与降级

  // 摘要生成失败不应影响聊天
  try {
    await generateSummary(conversationId)
  } catch (error) {
    // 记录失败，稍后重试
    await summaryQueue.add({ conversationId, retryCount: 0 })
    // 不抛出，聊天继续正常进行
  }

  2. 隐私与合规

  必须考虑：
  - 记忆数据加密存储（AES-256）
  - 用户删除账号时完全清除记忆
  - 提供"让 AI 忘记某些内容"的能力
  - 数据存储地点（国内合规）

  3. 多角色记忆隔离与共享

  当前方案按 (user_id, character_id) 隔离

  需要考虑：
  ├── 用户基础信息（姓名、生日）→ 可跨角色共享
  ├── 情感状态、对话习惯 → 可跨角色共享
  └── 特定角色的约定（"你答应陪我看电影"）→ 严格隔离

  4. 冷启动优化

  新用户没有记忆时，可以通过引导式对话快速建立：

  首次对话的 Prompt 增强：
  "这是你们的首次相遇，请自然地通过对话了解用户，
  记住他/她分享的关于自己的信息。"

  5. 监控与质量评估

  建议监控指标：
  - 摘要生成成功率
  - 平均摘要长度 / 信息密度
  - 向量搜索命中率（检索到的记忆是否被实际使用）
  - 用户反馈（"你怎么不记得了？"类负面信号）

  ---
  四、实现优先级建议

  Phase 1（MVP，1-2周）：
  ✅ 策略一基础版
  ✅ 对话结束时生成摘要
  ✅ 新对话注入摘要
  工作流程

  用户聊天中
      ↓
  发送消息 → sendMessage()
      ↓
  [注入记忆摘要到 system prompt]
      ↓
  生成 AI 回复
      ↓
  用户离开页面 
      ↓
  组件 detached() / 10轮（20条消息）对话 触发
      ↓
  调用 forceUpdateMemorySummary()
      ↓
  [新消息 ≥ 3 条时]
      ↓
  LLM 生成新摘要 → 存储到 memory_summary

  Phase 2（增强，2-3周）：
  ✅ 分层摘要（短期/中期/长期）
  ✅ 异步队列处理
  ✅ 失败重试机制

  Phase 3（进阶，按需）：
  ⬜ pgvector 向量搜索
  ⬜ 记忆碎片提取
  ⬜ 混合模式

